{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 02:41:14.154383: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 02:41:14.361188: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-19 02:41:14.361261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-19 02:41:14.368664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-19 02:41:14.389943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-19 02:41:19.915714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import keras\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/stackoverflow.json' #origial review documents, there are 50 classes\n",
    "with open(fn, 'r') as infile:\n",
    "        docs = json.load(infile)\n",
    "X = docs['X']\n",
    "y = np.asarray(docs['y'])\n",
    "num_classes = len(docs['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words:  18926\n"
     ]
    }
   ],
   "source": [
    "#count each word's occurance\n",
    "def count_word(X):\n",
    "    word_count = dict()\n",
    "    for d in X:\n",
    "        for w in d.lower().split(' '): #lower\n",
    "            if w in word_count:\n",
    "                word_count[w] += 1\n",
    "            else:\n",
    "                word_count[w] = 1            \n",
    "    return word_count\n",
    "\n",
    "word_count = count_word(X)\n",
    "print('total words: ', len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent word size =  1616\n"
     ]
    }
   ],
   "source": [
    "#get frequent words\n",
    "freq_words = [w  for w, c in word_count.items() if c > 10]\n",
    "print('frequent word size = ', len(freq_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word index\n",
    "word_to_idx = {w: i+2  for i, w in enumerate(freq_words)} # index 0 for padding, index 1 for unknown/rare words\n",
    "idx_to_word = {i:w for w, i in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_word(X):\n",
    "    seqs = []\n",
    "    max_length = 0\n",
    "    for d in X:\n",
    "        seq = []\n",
    "        for w in d.lower().split():\n",
    "            if w in word_to_idx:\n",
    "                seq.append(word_to_idx[w])\n",
    "            else:\n",
    "                seq.append(1) #rare word index = 1\n",
    "        seqs.append(seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index documents and pad each review to length = 3000\n",
    "indexed_X = index_word(X)\n",
    "padded_X = preprocessing.sequence.pad_sequences(indexed_X, maxlen=3000, dtype='int32', padding='post', truncating='post', value = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #split review into training and testing set\n",
    "# def splitTrainTest(X, y, ratio = 0.7): # 70% for training, 30% for testing\n",
    "#     shuffle_idx = np.random.permutation(len(y))\n",
    "#     split_idx = int(0.7*len(y))\n",
    "#     shuffled_X = X[shuffle_idx]\n",
    "#     shuffled_y = y[shuffle_idx]\n",
    "    \n",
    "#     return shuffled_X[:split_idx], shuffled_y[:split_idx], shuffled_X[split_idx:], shuffled_y[split_idx:]   \n",
    "\n",
    "# train_X, train_y, test_X, test_y = splitTrainTest(padded_X, y)\n",
    "\n",
    "# print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13994, 3000) (13994,) (5991, 3000) (5991,)\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.25\n",
    "train_X,  test_X = padded_X[docs[f\"X_{ratio}_train\"]], padded_X[docs[f\"X_{ratio}_test\"]]\n",
    "train_y,  test_y = y[docs[f\"X_{ratio}_train\"]], y[docs[f\"X_{ratio}_test\"]]\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#split reviews into seen classes and unseen classes\n",
    "def splitSeenUnseen(X, y, seen, unseen):\n",
    "    seen_mask = np.in1d(y, seen)# find examples whose label is in seen classes\n",
    "    unseen_mask = np.in1d(y, unseen)# find examples whose label is in unseen classes\n",
    "    \n",
    "    print(np.array_equal(np.logical_and(seen_mask, unseen_mask), np.zeros((y.shape), dtype= bool)))#expect to see 'True', check two masks are exclusive\n",
    "    \n",
    "    # map elements in y to [0, ..., len(seen)] based on seen, map y to unseen_label when it belongs to unseen classes\n",
    "    to_seen = {l:i for i, l in enumerate(seen)}\n",
    "    unseen_label = len(seen)\n",
    "    to_unseen = {l:unseen_label for l in unseen}\n",
    "        \n",
    "    return X[seen_mask], np.vectorize(to_seen.get)(y[seen_mask]), X[unseen_mask], np.vectorize(to_unseen.get)(y[unseen_mask])\n",
    "\n",
    "# seen = range(25)#seen classes\n",
    "# unseen = range(25,50)#unseen classes\n",
    "seen = docs[f\"target_{ratio}\"]\n",
    "unseen = [i for i in range(num_classes) if i not in seen]#unseen classes\n",
    "\n",
    "seen_train_X, seen_train_y, _, _ = splitSeenUnseen(train_X, train_y, seen, unseen)\n",
    "seen_test_X, seen_test_y, unseen_test_X, unseen_test_y = splitSeenUnseen(test_X, test_y, seen, unseen)\n",
    "\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "cate_seen_train_y = to_categorical(seen_train_y, len(seen))#make train y to categorial/one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network, in the paper, I use pretrained google news embedding, here I do not use it and set the embedding layer trainable\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "def Network(MAX_SEQUENCE_LENGTH = 3000, EMBEDDING_DIM = 300, nb_word = len(word_to_idx)+2, filter_lengths = [3, 4, 5],\n",
    "    nb_filter = 150, hidden_dims =250):\n",
    "    \n",
    "    graph_in = Input(shape=(MAX_SEQUENCE_LENGTH,  EMBEDDING_DIM))\n",
    "    convs = []\n",
    "    for fsz in filter_lengths:\n",
    "        conv = Conv1D(filters=nb_filter,\n",
    "                                 kernel_size=fsz,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu')(graph_in)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        convs.append(pool)\n",
    "\n",
    "    if len(filter_lengths)>1:\n",
    "        out = Concatenate(axis=-1)(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "\n",
    "    graph = Model(inputs=graph_in, outputs=out) #convolution layers\n",
    "    \n",
    "    emb_layer = [Embedding(nb_word,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True),\n",
    "                 Dropout(0.2)\n",
    "        ]\n",
    "    conv_layer = [\n",
    "            graph,\n",
    "        ]\n",
    "    feature_layers1 = [\n",
    "            Dense(hidden_dims),\n",
    "            Dropout(0.2),\n",
    "            Activation('relu')\n",
    "    ]\n",
    "    feature_layers2 = [\n",
    "            Dense(len(seen) ),\n",
    "            Dropout(0.2),\n",
    "    ]\n",
    "    output_layer = [\n",
    "            Activation('sigmoid')\n",
    "    ]\n",
    "\n",
    "    model = Sequential(emb_layer+conv_layer+feature_layers1+feature_layers2+output_layer)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 02:41:28.224586: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3000, 300)         485400    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3000, 300)         0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 450)               540450    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               112750    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 250)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1255      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1139855 (4.35 MB)\n",
      "Trainable params: 1139855 (4.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Network()    \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' embedding/embeddings:0': Shape mismatch.The variable shape (1618, 300), and the assigned value shape (339, 300) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m early_stopping\u001b[38;5;241m=\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model.fit(seen_train_X, cate_seen_train_y, epochs=2, batch_size=128, callbacks=[checkpointer, early_stopping], validation_split=0.2)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbestmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.10/site-packages/keras/src/backend.py:4359\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[0;34m(variable, value)\u001b[0m\n\u001b[1;32m   4356\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[1;32m   4357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4358\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[0;32m-> 4359\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign value to variable ' embedding/embeddings:0': Shape mismatch.The variable shape (1618, 300), and the assigned value shape (339, 300) are incompatible."
     ]
    }
   ],
   "source": [
    "bestmodel_path = '/data/chenxi/research/conference/LLM4OpenTC/baselines/EMNLP2017_DOC/ckpt/atis/0.25/0.h5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=bestmodel_path, verbose=1, save_best_only=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# model.fit(seen_train_X, cate_seen_train_y, epochs=2, batch_size=128, callbacks=[checkpointer, early_stopping], validation_split=0.2)\n",
    "\n",
    "model.load_weights(bestmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 2s 14ms/step\n",
      "(3497, 5)\n"
     ]
    }
   ],
   "source": [
    "seen_train_X_pred = model.predict(seen_train_X)\n",
    "print(seen_train_X_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a gaussian model\n",
    "from scipy.stats import norm as dist_model\n",
    "def fit(prob_pos_X):\n",
    "    prob_pos = [p for p in prob_pos_X]+[2-p for p in prob_pos_X]\n",
    "    pos_mu, pos_std = dist_model.fit(prob_pos)\n",
    "    return pos_mu, pos_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.5076562138892378], [1.0, 0.5995021533323615], [1.0, 0.3680115145534349], [1.0, 0.5334186965707056], [1.0, 0.3524866552827439]]\n"
     ]
    }
   ],
   "source": [
    "#calculate mu, std of each seen class\n",
    "mu_stds = []\n",
    "for i in range(len(seen)):\n",
    "    pos_mu, pos_std = fit(seen_train_X_pred[seen_train_y==i, i])\n",
    "    mu_stds.append([pos_mu, pos_std])\n",
    "\n",
    "print(mu_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 2s 10ms/step\n",
      "(5991, 5) (5991,)\n"
     ]
    }
   ],
   "source": [
    "#predict on test examples\n",
    "test_X_pred = model.predict(np.concatenate([seen_test_X,unseen_test_X], axis = 0))\n",
    "test_y_gt = np.concatenate([seen_test_y,unseen_test_y], axis = 0)\n",
    "print(test_X_pred.shape, test_y_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get prediction based on threshold\n",
    "test_y_pred = []\n",
    "scale = 1.\n",
    "for p in test_X_pred:# loop every test prediction\n",
    "    max_class = np.argmax(p)# predicted class\n",
    "    max_value = np.max(p)# predicted probability\n",
    "    threshold = max(0.5, 1. - scale * mu_stds[max_class][1])#find threshold for the predicted class\n",
    "    if max_value > threshold:\n",
    "        test_y_pred.append(max_class)#predicted probability is greater than threshold, accept\n",
    "    else:\n",
    "        test_y_pred.append(len(seen))#otherwise, reject\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.50      0.66       300\n",
      "           1       0.92      0.04      0.07       300\n",
      "           2       0.95      0.69      0.80       299\n",
      "           3       0.99      0.49      0.65       300\n",
      "           4       1.00      0.72      0.84       299\n",
      "           5       0.85      1.00      0.92      4493\n",
      "\n",
      "    accuracy                           0.87      5991\n",
      "   macro avg       0.95      0.57      0.66      5991\n",
      "weighted avg       0.88      0.87      0.84      5991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y_gt, test_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
