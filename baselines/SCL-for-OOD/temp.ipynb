{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont: 0\n",
      "train : valid : test = 2172 : 433 : 777\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_gui\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import metrics\n",
    "from thop import profile\n",
    "# Modeling\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "from model import BiLSTM\n",
    "from model import PGD_contrastive\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "dataset = 'atis'\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQ_LEN = 100\n",
    "unseen_proportion = 100\n",
    "cont_proportion = 1.0\n",
    "mask_proportion = 0\n",
    "proportion = 50\n",
    "\n",
    "df, partition_to_n_row = load_data(dataset)\n",
    "\n",
    "df['content_words'] = df['text'].apply(lambda s: word_tokenize(s))\n",
    "texts = df['content_words'].apply(lambda l: \" \".join(l))\n",
    "\n",
    "# Do not filter out \",\" and \".\"\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<UNK>\", filters='!\"#$%&()*+-/:;<=>@[\\]^_`{|}~')\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequences_pad = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Train-valid-test split\n",
    "idx_train = (None, partition_to_n_row['train'])\n",
    "idx_valid = (partition_to_n_row['train'], partition_to_n_row['train'] + partition_to_n_row['valid'])\n",
    "idx_test = (partition_to_n_row['train'] + partition_to_n_row['valid'], partition_to_n_row['train'] + partition_to_n_row['valid'] + partition_to_n_row['test'])\n",
    "idx_cont = (partition_to_n_row['train'] + partition_to_n_row['valid'] + partition_to_n_row['test'], None)\n",
    "\n",
    "X_train = sequences_pad[idx_train[0]:idx_train[1]]\n",
    "X_valid = sequences_pad[idx_valid[0]:idx_valid[1]]\n",
    "X_test = sequences_pad[idx_test[0]:idx_test[1]]\n",
    "X_cont = sequences_pad[idx_cont[0]:idx_cont[1]]\n",
    "\n",
    "df_train = df[idx_train[0]:idx_train[1]]\n",
    "df_valid = df[idx_valid[0]:idx_valid[1]]\n",
    "df_test = df[idx_test[0]:idx_test[1]]\n",
    "df_cont = df[idx_cont[0]:idx_cont[1]]\n",
    "\n",
    "y_train = df_train.label.reset_index(drop=True)\n",
    "y_valid = df_valid.label.reset_index(drop=True)\n",
    "y_test = df_test.label.reset_index(drop=True)\n",
    "y_cont = df_cont.label.reset_index(drop=True)\n",
    "train_text = df_train.text.reset_index(drop=True)\n",
    "valid_text = df_valid.text.reset_index(drop=True)\n",
    "test_text = df_test.text.reset_index(drop=True)\n",
    "cont_text = df_cont.text.reset_index(drop=True)\n",
    "print(\"cont: %d\" % (X_cont.shape[0]))\n",
    "\n",
    "n_class = y_train.unique().shape[0]\n",
    "\n",
    "\n",
    "## 我自己设置的固定已知类\n",
    "y_cols_seen = pd.read_csv(f'data/{dataset}/known_class_{proportion}.txt', header=None)[0].tolist()\n",
    "y_cols_unseen = list(set(y_train.value_counts().index) - set(y_cols_seen))\n",
    "n_class_seen = len(y_cols_seen)\n",
    "##\n",
    "\n",
    "y_cols_unseen_b = []\n",
    "\n",
    "for i in range(len(y_cols_seen)):\n",
    "    tmp_idx = y_train[y_train.isin([y_cols_seen[i]])]\n",
    "    tmp_idx = tmp_idx[:int(proportion / 100 * len(tmp_idx))].index\n",
    "    if not i:\n",
    "        part_train_seen_idx = tmp_idx\n",
    "    else:\n",
    "        part_train_seen_idx = np.concatenate((part_train_seen_idx, tmp_idx), axis=0)\n",
    "\n",
    "train_seen_idx = y_train[y_train.isin(y_cols_seen)].index\n",
    "train_ood_idx = y_train[y_train.isin(y_cols_unseen)]\n",
    "train_ood_idx = train_ood_idx[:int(unseen_proportion / 100 * len(train_ood_idx))].index\n",
    "\n",
    "valid_seen_idx = y_valid[y_valid.isin(y_cols_seen)].index\n",
    "valid_ood_idx = y_valid[y_valid.isin(y_cols_unseen)]\n",
    "valid_ood_idx = valid_ood_idx[:int(unseen_proportion / 100 * len(valid_ood_idx))].index\n",
    "\n",
    "test_seen_idx = y_test[y_test.isin(y_cols_seen)].index\n",
    "test_ood_idx = y_test[y_test.isin(y_cols_unseen)].index\n",
    "\n",
    "src_cols = ['src']\n",
    "bt_cols = ['bt']\n",
    "src_idx = y_cont[y_cont.isin(src_cols)]\n",
    "ind_src_idx = src_idx[:int(cont_proportion * 0.8 * len(src_idx))].index\n",
    "ood_src_idx = src_idx[int(0.8 * len(src_idx)):int(0.8 * len(src_idx) + cont_proportion * 0.2 * len(src_idx))].index\n",
    "bt_idx = y_cont[y_cont.isin(bt_cols)]\n",
    "ind_bt_idx = bt_idx[:int(cont_proportion * 0.8 * len(bt_idx))].index\n",
    "ood_bt_idx = bt_idx[int(0.8 * len(bt_idx)):int(0.8 * len(bt_idx) + cont_proportion * 0.2 * len(bt_idx))].index\n",
    "\n",
    "X_train_seen = X_train[part_train_seen_idx]\n",
    "X_train_ood = X_train[train_ood_idx]\n",
    "y_train_seen = y_train[part_train_seen_idx]\n",
    "train_seen_text = list(train_text[part_train_seen_idx])\n",
    "train_unseen_text = list(train_text[train_ood_idx])\n",
    "X_valid_seen = X_valid[valid_seen_idx]\n",
    "X_valid_ood = X_valid[valid_ood_idx]\n",
    "y_valid_seen = y_valid[valid_seen_idx]\n",
    "valid_seen_text = list(valid_text[valid_seen_idx])\n",
    "valid_unseen_text = list(valid_text[valid_ood_idx])\n",
    "X_test_seen = X_test[test_seen_idx]\n",
    "X_test_ood = X_test[test_ood_idx]\n",
    "y_test_seen = y_test[test_seen_idx]\n",
    "test_seen_text = list(test_text[test_seen_idx])\n",
    "test_unseen_text = list(test_text[test_ood_idx])\n",
    "\n",
    "print(\"train : valid : test = %d : %d : %d\" % (X_train_seen.shape[0], X_valid_seen.shape[0], X_test_seen.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_ind_x = X_cont[ind_src_idx]\n",
    "src_ind_y = y_cont[ind_src_idx]\n",
    "bt_ind_x = X_cont[ind_bt_idx]\n",
    "bt_ind_y = y_cont[ind_bt_idx]\n",
    "src_ood_x = X_cont[ood_src_idx]\n",
    "src_ood_y = y_cont[ood_src_idx]\n",
    "bt_ood_x = X_cont[ood_bt_idx]\n",
    "bt_ood_y = y_cont[ood_bt_idx]\n",
    "\n",
    "if y_cols_unseen_b:\n",
    "    train_ood_idx_b = y_train[y_train.isin(y_cols_unseen_b)].index\n",
    "    X_train_ood_b = X_train[train_ood_idx_b]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train_seen)\n",
    "y_train_idx = le.transform(y_train_seen)\n",
    "y_valid_idx = le.transform(y_valid_seen)\n",
    "y_test_idx = le.transform(y_test_seen)\n",
    "ood_index = y_test_idx[0]\n",
    "y_train_onehot = to_categorical(y_train_idx)\n",
    "y_valid_onehot = to_categorical(y_valid_idx)\n",
    "y_test_onehot = to_categorical(y_test_idx)\n",
    "\n",
    "for i in range(int(mask_proportion / 100 * len(y_train_onehot))):\n",
    "    y_train_onehot[i] = [0.0] * n_class_seen\n",
    "\n",
    "y_train_ood = np.array([[0.0] * n_class_seen for _ in range(len(train_ood_idx))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47      atis_abbreviation\n",
       "55      atis_abbreviation\n",
       "70      atis_abbreviation\n",
       "142     atis_abbreviation\n",
       "223     atis_abbreviation\n",
       "              ...        \n",
       "1436        atis_quantity\n",
       "1611        atis_quantity\n",
       "1612        atis_quantity\n",
       "1619        atis_quantity\n",
       "1730        atis_quantity\n",
       "Name: label, Length: 2172, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_abbreviation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_airline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_cheapest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_flight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atis_flight_no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>atis_ground_fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>atis_ground_service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>atis_quantity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "0    atis_abbreviation\n",
       "1         atis_airline\n",
       "2        atis_cheapest\n",
       "3            atis_city\n",
       "4          atis_flight\n",
       "5       atis_flight_no\n",
       "6     atis_ground_fare\n",
       "7  atis_ground_service\n",
       "8        atis_quantity"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cols_seen = pd.read_csv(f'data/{dataset}/known_class_{proportion}.txt', header=None)\n",
    "# y_cols_unseen = list(set(y_train.value_counts().index) - set(y_cols_seen))\n",
    "# n_class_seen = len(y_cols_seen)\n",
    "y_cols_seen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
